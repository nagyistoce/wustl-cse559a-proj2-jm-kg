<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
    "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-type" content="text/html; charset=ISO-8859-1" />
<title>Face Recognition</title>

<style type="text/css">
table.s1{border-style:none;}
table.s2{border-collapse:collapse; border:1px solid black;}
table.s2 td{padding:4px; font-family: Courier New; font-size: 9pt; border:1px solid black;}
table.s2 th{padding:4px; font-family: Courier New; font-size: 10pt; border:1px solid black;}
body{font-family: Courier New; font-size: 10pt;}
h3{font-family: Courier New; font-size: 12pt;}
h4{font-family: Courier New; font-size: 11pt;}
</style>
</head>

<body>
<h1>Face Recognition</h1>
<h3>By Kurt Glastetter, Josh Mason</h3>
<p>Link to source: <a href="gm_pca-src.zip">Source</a><br/></p>

<table class="s1" title="Eigenfaces">
<tr> <td><img src="ef1.png" alt="eigenface 1"/></td> <td><img src="ef2.png" alt="eigenface 2"/></td> <td><img src="ef3.png" alt="eigenface 3"/></td> <td><img src="ef4.png" alt="eigenface 4"/></td> <td><img src="ef5.png" alt="eigenface 5"/></td></tr>
</table>


<h2>Description</h2>

<p>
For this project, we compared various techniques for face recognition.  We focused our efforts on Principal Component Analysis and Mean Face comparison.  We experimented using different pre-processing techniques before performing PCA as well as testing different measures of similarity and modifying the value of <i>k</i> in k-Nearest Neighbor.</p>

<p>Principal component analysis for face recognition works in the following way.  Given a set of images, find the mean image.  Subtract the mean image from each of the original images and create a two-dimensional matrix where each column represents an image and each row represents a pixel from the image.  Once the matrix has been formed, perform singular value decomposition (SVD) on this matrix.  The result is a matrix of basis images, a matrix of coefficients which define the shift in space from each basis image back to the original image minus the mean image, and a diagonal matrix which holds the weight of each of the coefficients.  It should be apparent from this that the number of coefficients obtained from SVD is the same as the number of basis images created.  A few examples of these basis images are shown at the top of this web page.  The coefficients found by PCA can then be used to perform image recognition by performing a machine learning techniques on the training set and attempting to match test images to the original training set.</p>

<p>It is worth noting here that we initially wrote up our results and had <i>completed</i> our project until shortly after class on March 23rd.  During class, Professor Pless commented on the fact that you could combine the S and V created during SVD into a single “weighted coefficients” matrix.  Curious if this would change our results if we compared our test images against the weighted coefficients instead of the standard coefficients, we tried it out and were extremely surprised by the results.  All of our results for the ATT data set went up significantly across the board.  However, our results for the ORL data set were mixed.  Because of this, this page is our results, round 2 style.  The write-up for the extension 2 section was completely re-written.  You can find the original web page with our original results <a href="orig.html">over here</a>.</p>

<p>We used k-Nearest Neighbor to attempt to match a face with the subject it most closely related to.  kNN works by finding the neighbors that are "closest" to a test image and picking the neighbor that is most frequent as its best match.  The value of <i>k</i> determines how many neighbors will be analyzed when trying to pick a match.  In order for kNN to work, a measure of similarity between the test image and all training images needs to be computed.  These results are then sorted and the top <i>k</i> neighbors are chosen.  The neighbors in our algorithm are the list of images that were the closest match to the test image. Because each person has multiple images for them, we determine which person showed up most frequently, and say that is who the test image most closely matches.</p>

<p>We tried four different measures of similarity before using its results to find the nearest neighbor. The similarity measures we tried using were:</p>
<ul>
	<li>Euclidean Distance (<a href="http://en.wikipedia.org/wiki/Euclidean_distance">info</a>)<br/><br/>

	Euclidean Distance (ED) performed the best compared to the other measures of similarity that we experimented with.
	<br/><br/></li>
	<li>Normalized Euclidean Distance (<a href="http://www.tsi.enst.fr/tsi/enseignement/ressources/mti/classif-textures/node4.html">info</a>)<br/><br/>
	Normalized Euclidean Distance (NED) performed exactly the same as ED did in all of our experiments.
	<br/><br/></li>
	<li>Cosine Similarity (<a href="http://en.wikipedia.org/wiki/Cosine_similarity">info</a>)<br/><br/>
	Cosine (CS) performed poorly in all of our experiments.  It's results are not shown in any of our tables below because it's results are not worth showing.  It performed better than chance, but only marginally.  It was not a good measure of similarity here.  This is not surprising as CS tends to be the preferred technique for comparing text and not faces or coefficients from PCA.
	<br/><br/></li>

	<li>Mahalanobis Distance (<a href="http://en.wikipedia.org/wiki/Mahalanobis_distance">info</a>)<br/><br/>
	Mahalanobis Distance (MD) performed extremely bad.  It performed as good as chance, but no better.  It's possible we implemented MD incorrectly or that MD really is just not a good measure of similarity for comparing coefficients.</li>
</ul>

<p>To test our implementation we used the <a href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html">AT&amp;T face database</a>.  We had two main directories: one with all the training images in it and another, with the same folder structure, which had the test faces.  For every subject/person from the original AT&amp;T face database, we randomly held back a single image from that person and placed it into the corresponding folder in the test directory.  We then read in all the files in the train set and performed PCA on set of images.  Next we read in all the faces from the test folder and, one-by-one, subtracted the mean face from the image and then projected it onto the basis images (the U matrix) and calculated its coefficients.  We then performed k-Nearest Neighbor to find the image(s) which most closely matched the ones from the test image.</p>

<p>Our mean face algorithm testing was similar to the above, using the same training data and test data.  The difference is that, as we read in the training set, we averaged together the set of images for each person individually, and stored all of these mean faces in memory.  Then, when reading the test set, we simply performed Euclidean Nearest Neighbor (k-Nearest Neighbor, but with <i>k=1</i>) to find the most similar mean face to the given test face.</p>


<h3>Initial Results</h3>
<p>Below is the table showing our initial results on the ATT face set with no pre-processing performed.  The table below shows the results when we used Euclidean distance as our measure of similarity and <i>k=1</i> for the value of <i>k</i> in k-Nearest Neighbor.</p>

<table class="s2" title="Euclidean Distance Results">
<caption>Initial Results</caption>
<tr> <th>Method</th>             <th>Correct</th>        <th>Incorrect</th>        <th>% Correct</th> </tr>
<tr> <td>PCA 2 Coefficients</td> <td>13</td>             <td>27</td>               <td>32.5</td> </tr>
<tr> <td>PCA 4 Coefficients</td> <td>33</td>             <td>7</td>                <td>82.5</td> </tr>
<tr> <td>PCA 6 Coefficients</td> <td>38</td>             <td>2</td>                <td>95.0</td> </tr>
<tr> <td>PCA 9 Coefficients</td> <td>39</td>             <td>1</td>                <td>97.5</td> </tr>
<tr> <td>Mean Face</td>          <td>39</td>             <td>1</td>                <td>97.5</td> </tr>
</table>

<h2>Extension 1 Description</h2>

<table class="s1" title="Famous Mean Faces">
<tr>
<td> <img src="mf-Andre_Agassi.jpg"          alt="Andre Agassi's mean face"          title="Andre Agassi's mean face"         /> </td>
<td> <img src="mf-Angelina_Jolie.jpg"        alt="Angelina Jolie's mean face"        title="Angelina Jolie's mean face"       /> </td>
<td> <img src="mf-Arnold_Schwarzenegger.jpg" alt="Arnold Schwarzenegger's mean face" title="Arnold Schwarzenegger's mean face"/> </td>
<td> <img src="mf-Bill_Clinton.jpg"          alt="Bill Clinton's mean face"          title="Bill Clinton's mean face"         /> </td>
<td> <img src="mf-Winona_Ryder.jpg"          alt="Winona Ryder's mean face"          title="Winona Ryder's mean face"         /> </td>
</tr>
</table>
<p>We extended the base option to work with another data set of faces; specifically, the 40-person subset of <a href="http://www.openu.ac.il/home/hassner/data/lfwa/">aligned labeled faces in the wild (LFW-a)</a>.  The images above show the results of averaging together a few images of individual famous people from this data set, resulting in faces that are notably still recognizable to humans.</p>
<p>We tested this data by using the full set of images as training data (with different numbers of images per person), and also by limiting the training data to 18 randomly selected images per person (such that each person has equal representation in the training data), with both PCA recognition and mean face recognition.</p>
<p>For each of these training data options, we tried using full images (but resampled to half-size, due to computer memory constraints), and using only the face portion of the image (the 70x100 center rectangle, highlighted in the images above).  Using the full images produced very poor results in all tests, so that data is not shown.  This is probably because much of the background portions of the images were evaluated in the matching, which contributes to erroneous results.  Using only the central face portion of the image produced much better results, but still markedly worse than that for the more homogeneous data set in the base option.  In fact, the best results are significantly less than half correct.  The poor results are probably due to the fact that the images vary so much more than the ones from base option.  Even with only the face portion shown, the lighting conditions are still greatly varying compared to that of base option, resulting in poor matching.</p>
<p>Limiting the data set to an equal number of images per person yielded somewhat better results for PCA.  In contrast, mean face results got slightly worse when the data set was limited, but mean face results were always significantly better than PCA results (for the tested numbers of coefficients, at least).  Unfortunately, none of these results are very good.</p>

<h3>Extension 1 Results</h3>
<p>Below is the table showing our overall results for our testing on the aligned labeled faces in the wild image set.  This uses only the center 70x100 face portion of the image, with <i>k=1</i>.</p>
<table class="s2" title="Aligned Labeled Faces (LFW-a) Results">
<caption>Aligned Labeled Faces (LFW-a) Results</caption>
<tr> <th>Method</th>             <th>Correct</th>        <th>Incorrect</th>        <th>% Correct</th> </tr>
<tr> <td>PCA 2 Coefficients</td> <td>0</td>              <td>46</td>               <td>0.0</td>  </tr>
<tr> <td>PCA 4 Coefficients</td> <td>0</td>              <td>46</td>               <td>0.0</td>  </tr>
<tr> <td>PCA 6 Coefficients</td> <td>2</td>              <td>44</td>               <td>4.4</td>  </tr>
<tr> <td>PCA 9 Coefficients</td> <td>8</td>              <td>38</td>               <td>17.3</td> </tr>
<tr> <td>Mean Face</td>          <td>18</td>             <td>28</td>               <td>39.1</td> </tr>
</table>

<p>Below is the table showing our overall results for testing the aligned labeled faces in the wild, but by training with equal-sized data sets for each subject (18 images each).  The training images for each subject were chosen at random.  Again, only the center 70x100 face portion of the image was used, with <i>k=1</i>.</p>
<table class="s2" title="LFW-a Equal Sized Sets Results">
<caption>LFW-a Equal Sized Sets Results</caption>
<tr> <th>Method</th>             <th>Correct</th>        <th>Incorrect</th>        <th>% Correct</th> </tr>
<tr> <td>PCA 2 Coefficients</td> <td>2</td>              <td>44</td>               <td>4.4</td>  </tr>
<tr> <td>PCA 4 Coefficients</td> <td>6</td>              <td>40</td>               <td>13.0</td>  </tr>
<tr> <td>PCA 6 Coefficients</td> <td>5</td>              <td>41</td>               <td>10.9</td>  </tr>
<tr> <td>PCA 9 Coefficients</td> <td>9</td>              <td>37</td>               <td>19.6</td> </tr>
<tr> <td>Mean Face</td>          <td>16</td>             <td>30</td>               <td>34.8</td> </tr>
</table>

<h2>Extension 2 Description</h2>

<table class="s1">
<tr> 
	<td><img src="im1.png" alt="original image" title="original image"/></td> 
	<td><img src="edgemap.png" alt="edge map" title="edge map"/></td> 
	<td><img src="overlaidedgemap.png" alt="edge map overlay" title="edge map overlay"/></td> 
	<td><img src="histeq.png" alt="equalized histogram" title="equalized histogram"/></td> 
	<td><img src="overlaidedgemaphisteq.png" alt="equalized histogram w/overlaid edge map" title="equalized histogram w/overlaid edge map"/></td> 
	<td><img src="overlaidedgemaphisteq2.png" alt="equalized histogram w/original edge map overlaid" title="equalized histogram w/original edge map overlaid"/></td>
</tr>
</table>

<p>The images above show the various pre-processing techniques which were used prior to performing PCA on the image set.  From left to right, the images above are: the original image; the edge map of the original image; the edge map overlaid on the original image; the original image with the histogram equalized; histogram equalized image with edge map obtained from histogram qualized image overlaid; histogram equalized image with edge map obtained from original image overlaid.</p>

<p>The first pre-processing technique we tried was to calculate the edge map of the original image.  We used the built in Matlab <i>edge</i> function to calculate the edge map.  We thought that by reducing the image down to the edge map, we might be able to reduce the time needed by PCA to compute the coefficients.  We also believed it would lead to better results because it would reduce the face down to a few key features that would distinguish it from other faces.  Surprisingly, the use of edge map caused the number of correct matches to be greatly reduced.  We theorize that this reduction in correct matches is due to a reduction in the amount of image data describing the face.    Because there is less data describing the face, the differences between one face and another is decreased, leaving PCA to generate similar coefficients for dissimilar faces. See the <a href="#e2r1">Edge Map Results</a> table below for results of this experiment.</p>

<p>Because of this observation, we thought that if we tried overlaying the edge map on top of the original image, we might be able to boost our initial results.  We believed that retaining the original image data and making edges more apparent might help to distinguish faces from one  another.  This experiment proved to provide better results than when just the edge map along was used.  It also improved results over our base test, with no pre-processing done, for 2.  For 4, 6 and 9, the results decreased slightly.  We speculate the increase in success over just edge detection is due to additional detail about the image being retained prior to performing PCA.  This additional info helps to better separate the distinguishing features between each face and helps lead to better results when performing recognition. It's interesting that this technique helped improve our results only partially over our original results.  For two coefficients, it's possible that the addition of the edge map helped better distinguish the faces, but when 4, 6 and 9 coefficients where used, it caused ambiguity among the faces and caused them to be incorrectly matched.  See the <a href="#e2r2">Overlaid Edge Map Results</a> table below for results of this experiment.</p>

<p>Not entirely happy with the previous results, we tried another pre-processing technique and thought we could improve a bit more.  This time we tried histogram equalization on the entire image.  Histogram equalization is a process used to increase the pixel intensity of the image to match some defined histogram.  Histogram equalization can have the benefit of making less noticeable features more distinguishable.  For all of our tests, we used histogram equalization with a flat histogram.  This had the overall effect of boosting the contrast in our images.  We anticipated that this increase in detail for each image would increase our overall success, unfortunately, it had exactly the opposite effect.  Our results went down for all sets of coefficients.  The results are better than using just the edge map to describe the face, but worse than using the original face and worse than the edge map overlaid on the original face.  We theorize that histogram equalization causes the contrast in some areas of the image to be increased too much and results in a loss of detail for that section.  This loss in detail makes it more ambiguous to distinguish between faces and results in poor accuracy.  See the <a href="#e2r3">Histogram Equalization Results</a> table below for results of this experiment.</p>

<p>We thought that if overlaying an edge map on the original image was able to boost results, maybe we could boost our results of histogram equalization by overlaying an edge map on it.  First we equalized the image and then we took the edge map of the equalized image and lastly we overlaid the edge map on the equalized image.  This lead to a mixed bag of results.  Again our results did not approach our original, untouched images, but it did improve over simple edge detection. See the <a href="#e2r4">Histogram Equalization with Edge Map Results</a> table below for results of this experiment.</p>

<p>We realized, that taking the edge map of the histogram equalized edge may have resulted in some edges that were lost because they became "washed out" when the overall contrast of the image was increased.  So we decided to overlay the original edge map on top of the histogram equalized image to see what would happen.  Unfortunately again, our results weren't quite what we were expecting, they stayed roughly the same as the previous experiment.  The results again decreased over the original, base, results.  See the <a href="#e2r5">Histogram Equalization with Original Edge Map Results</a> table below for results of this experiment.</p>

<p>It's unfortunate that none of our pre-processing techniques were able to boost the results over our initial results.  When we did our first write-up (<a href="orig.html">see here</a>), all of these techniques were able to boost our results slightly except for just the edge map.  It's possible that all of these techniques removed data from the image that was used as part of the PCA analysis, instead of adding image data like we had hoped. This reduction in the amount of image data, or the ambiguity introduced by the techniques used, resulted in a drop in accuracy from the original, un-touched images.</p>

<h3>Extension 2 Results</h3>
<div id="e2r1">
<p>Below is the table showing our overall results when using edge map as a preprocessing step. The results were obtained using Euclidean distance and <i>k=1</i>.</p>
<table class="s2" title="Edge Map Results">
<caption>Edge Map Results</caption>
<tr> <th>Method</th>             <th>Correct</th>        <th>Incorrect</th>        <th>% Correct</th> </tr>
<tr> <td>PCA 2 Coefficients</td> <td>2</td>              <td>38</td>               <td>5.0</td> </tr>
<tr> <td>PCA 4 Coefficients</td> <td>5</td>              <td>35</td>               <td>12.5</td> </tr>
<tr> <td>PCA 6 Coefficients</td> <td>12</td>             <td>28</td>               <td>30.0</td> </tr>
<tr> <td>PCA 9 Coefficients</td> <td>13</td>             <td>27</td>               <td>32.5</td> </tr>
</table>
</div>

<div id="e2r2">
<p>Below is the table showing our overall results when using an edge map overlaid on the original image. The results were obtained using Euclidean distance and <i>k=1</i>.</p>
<table class="s2" title="Overlaid Edge Map Results">
<caption>Overlaid Edge Map Results</caption>
<tr> <th>Method</th>             <th>Correct</th>        <th>Incorrect</th>        <th>% Correct</th> </tr>
<tr> <td>PCA 2 Coefficients</td> <td>17</td>             <td>23</td>               <td>42.5</td> </tr>
<tr> <td>PCA 4 Coefficients</td> <td>27</td>             <td>13</td>               <td>67.5</td> </tr>
<tr> <td>PCA 6 Coefficients</td> <td>35</td>             <td>5</td>                <td>87.5</td> </tr>
<tr> <td>PCA 9 Coefficients</td> <td>37</td>             <td>3</td>                <td>92.5</td> </tr>
</table>
</div>

<div id="e2r3">
<p>Below is the table showing our overall results when using histogram equalization as a preprocessing step. The results were obtained using Euclidean distance and <i>k=1</i>.</p>
<table class="s2" title="Histogram Equalization Results">
<caption>Histogram Equalization Results</caption>
<tr> <th>Method</th>             <th>Correct</th>        <th>Incorrect</th>        <th>% Correct</th> </tr>
<tr> <td>PCA 2 Coefficients</td> <td>10</td>             <td>30</td>               <td>25.0</td> </tr>
<tr> <td>PCA 4 Coefficients</td> <td>26</td>             <td>14</td>               <td>65.0</td> </tr>
<tr> <td>PCA 6 Coefficients</td> <td>32</td>             <td>8</td>                <td>80.0</td> </tr>
<tr> <td>PCA 9 Coefficients</td> <td>37</td>             <td>3</td>                <td>92.5</td> </tr>
</table>
</div>

<div id="e2r4">
<p>Below is the table showing our overall results when using an edge map calculated from the histogram equalized image, overlaid on the histogram equalized image. The results were obtained using Euclidean distance and <i>k=1</i>.</p>
<table class="s2" title="Histogram Equalization with Edge Map Results">
<caption>Histogram Equalization with Edge Map Results</caption>
<tr> <th>Method</th>             <th>Correct</th>        <th>Incorrect</th>        <th>% Correct</th> </tr>
<tr> <td>PCA 2 Coefficients</td> <td>11</td>             <td>29</td>               <td>27.5</td> </tr>
<tr> <td>PCA 4 Coefficients</td> <td>25</td>             <td>15</td>               <td>62.5</td> </tr>
<tr> <td>PCA 6 Coefficients</td> <td>34</td>             <td>6</td>                <td>85.0</td> </tr>
<tr> <td>PCA 9 Coefficients</td> <td>35</td>             <td>5</td>                <td>87.5</td> </tr>
</table>
</div>

<div id="e2r5">
<p>Below is the table showing our overall results when using an edge map calculated from the original image, overlaid on the histogram equalized image. The results were obtained using Euclidean distance and <i>k=1</i>.</p>
<table class="s2" title="Histogram Equalization with Original Edge Map Results">
<caption>Histogram Equalization with Original Edge Map Results</caption>
<tr> <th>Method</th>             <th>Correct</th>        <th>Incorrect</th>        <th>% Correct</th> </tr>
<tr> <td>PCA 2 Coefficients</td> <td>9</td>              <td>31</td>               <td>22.5</td> </tr>
<tr> <td>PCA 4 Coefficients</td> <td>26</td>             <td>14</td>               <td>65.0</td> </tr>
<tr> <td>PCA 6 Coefficients</td> <td>34</td>             <td>6</td>                <td>85.0</td> </tr>
<tr> <td>PCA 9 Coefficients</td> <td>35</td>             <td>5</td>                <td>87.5</td> </tr>
</table>
</div>

<div id="e2r6">
<p>We tried many combinations of similarity measures and modifying the value of <i>k</i>.  The table below shows the top 5 combinations obtained.  All the results below are for PCA only.  Since Euclidean distance was the best measurement of similarity, all the results below were obtained using Euclidean distance as the similarity measurement.</p>
<table class="s2" title="Top Results">
<caption>Top Results</caption>
<tr> <th>Method</th>             <th>Correct</th>        <th>Incorrect</th>        <th>% Correct</th> </tr>
<tr> <td>c=9,k=1,e</td>          <td>37</td>             <td>3</td>                <td>92.5</td> </tr>
<tr> <td>c=9,k=1,h</td>          <td>37</td>             <td>3</td>                <td>92.5</td> </tr>
<tr> <td>c=9,k=1,he</td>         <td>35</td>             <td>5</td>                <td>87.5</td> </tr>
<tr> <td>c=9,k=1,he2</td>        <td>35</td>             <td>5</td>                <td>87.5</td> </tr>
<tr> <td>c=6,k=1,e</td>          <td>35</td>             <td>5</td>                <td>87.5</td> </tr>
</table>
<p>In the table above, c=# of coefficients used, h=histogram equalization, e=edge map overlaid on image, he=histogram equalization w/performed, then edge map overlaid on top, he2=histogram equalization w/performed, then the original edge map overlaid on top, k=# of neighbors examined.</p>

<p>It's interesting that all of our best results occurred when we used a value of 1 for <i>k</i>.  This could be that all images are very close to each other and when a larger neighborhood size is used, faces that have similar lighting or posture start being chosen than ones that are of the same face.  This ends up resulting in a decrease in results.</p>
</div>

<hr/>
<p><a href="http://validator.w3.org/check?uri=referer"><img
    src="http://www.w3.org/Icons/valid-xhtml11-blue" alt="Valid XHTML 1.1"
    height="31" width="88"/></a></p>
</body>

</html>
